{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. How Surprise works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Specifying How to read the file through Reader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(line_format='item user rating timestamp', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reader class will automatically re arranges the indices to form tuples ie.,\n",
    "# (user, item, rating, timestamp) irrespective of what our order is...\n",
    "reader.indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load the dataset from file (sample.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  by using Reader object, which knows how the data is stored in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD the dataset.....\n",
    "data = Dataset.load_from_file('sample.csv',reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sample.csv',\n",
       " [('1013034', '14890', 5.0, '2005-07-07'),\n",
       "  ('413056', '17169', 5.0, '2005-06-14'),\n",
       "  ('823022', '9526', 3.0, '2003-04-01')],\n",
       " 100000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the dataset, literally means making tuples of our data..\n",
    "data.ratings_file, data.raw_ratings[:3], len(data.raw_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Preparing Dataset :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Creating the Trainset and Testset Object:\n",
    "- If you want to split the data into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Now let's make Training and Testing Set to train our algorithm....\n",
    "\n",
    "\n",
    "* It returns a Trainset object... and testset( list of tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Trainingset is a Trainset class object that store various information along with user item rating, which is useful for furthur computations **.\n",
    "\n",
    "\n",
    "* **Trainset** has some  ***defaultdict's of user and item***. A dictionary of UserIds(replaced with indeces of ordering) that contains list of (item, ratings) pairs. Even Item Id's are also \n",
    " replaced with indices (in the order of occurance,)\n",
    "\n",
    "\n",
    " * But the data is not corrupted by doing this. It preserves same User-Item\n",
    "as before. But now with new Indices...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "surprise.trainset.Trainset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [(0, 4.0)]),\n",
       " (1, [(1, 3.0), (6014, 3.0)]),\n",
       " (2, [(2, 4.0), (1598, 3.0)]),\n",
       " (3, [(3, 3.0), (1028, 5.0)]),\n",
       " (4, [(4, 5.0)]),\n",
       " (5, [(5, 2.0), (1954, 5.0), (2517, 4.0)]),\n",
       " (6, [(6, 2.0), (1349, 3.0)]),\n",
       " (7, [(7, 4.0)]),\n",
       " (8, [(8, 4.0)]),\n",
       " (9, [(9, 3.0), (4202, 5.0)])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trainset.ur.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you want to know abou any user, we can get them by indexing defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 5.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.ur[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *Here is how the Testset looks like* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, [('286106', '13795', 4.0), ('1544601', '1307', 2.0)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (user, item, rating) pairs\n",
    "\n",
    "type(testset), testset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Creating only Trainset:\n",
    "- If you want to train with all the data in input file\n",
    "\n",
    "- It is done only when you have saperate file for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_1 = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The training set structure is same as above ( some info along with default dicts of user and item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can get that mappings of users and items to inner_item_ids and inner_user_ids***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_mapping: [('548549', 0), ('2411423', 1), ('118552', 2)]\n",
      "Item_mapping: [('4745', 0), ('4171', 1), ('14185', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 'OriginalItemIds' to 'NewItemIds' are stored in \"_raw2inner_items\" \n",
    "# Similarly, The mappings from 'OriginalUserIds' to 'NewUserIds' are like..\n",
    "\n",
    "print('User_mapping:',list(trainset._raw2inner_id_users.items())[:3])\n",
    "print('Item_mapping:',list(trainset._raw2inner_id_items.items())[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Some info that a Trainingset has ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.6051466666666667, 8367, True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It has several useful methods that are userful while training the model..\n",
    "# Example:\n",
    "trainset.global_mean, trainset.n_items, trainset.knows_item(3241)\n",
    "# and much more, where they are used inside the library while training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Run the SVD algorithm on the data (Trainingset Object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================\n",
    "### A sneak Peek in the algorithm\n",
    "\n",
    "- It is not purely SVD, it is **inspired from SVD**.\n",
    "\n",
    "\n",
    "- The standard SVD is modified for this Rating Prediction problem, by considering user biases, Item biases to make this new SVD. It uses **SGD** to solve optimization problem..(** With Regularization**)\n",
    "\n",
    "\n",
    "- In this we will solve the optimization problem of minimiZing the error by considering Biases of user and Item..\n",
    "\n",
    "http://www.albertauyeung.com/post/python-matrix-factorization/\n",
    "\n",
    "**Implementation is completely different in the above blogpost, But the concept is same**\n",
    "\n",
    "> ***Our Predicted Rating can be calculated from the following***\n",
    "\n",
    "\\begin{aligned}\n",
    "\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^Tp_u\n",
    "\\end{aligned}\n",
    "\n",
    "=============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import algorithm.....\n",
    "from surprise import SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lr_all : Learning rate for all hyperparameters.. (can make diffrnt lrngrates for diffrnt param)\n",
    "\n",
    "\n",
    "* reg_all: Regularization term for all parameters. (Can make diffrnt RegParams for diffrnt param)\n",
    "\n",
    "\n",
    "* with default parameters... (lr_all and reg_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can make unbiased SVD also....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1cf240bdf98>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SVD(random_state=15, biased=True, verbose=True, lr_all=0.007, reg_all=0.02) \n",
    "algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **20 is the defalut no of epochs for SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1cf240bdf98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Train the model on the dataset (trainset)\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Model Evaluation using RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 We can make it very easy if we have created testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid='286106', iid='13795', r_ui=4.0, est=3.4096806037322622, details={'was_impossible': False})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('286106', '13795', 4.0),\n",
       " ('1544601', '1307', 2.0),\n",
       " ('1149601', '14909', 4.0),\n",
       " ('709168', '1144', 3.0),\n",
       " ('2580101', '3224', 5.0),\n",
       " ('429701', '17671', 4.0),\n",
       " ('1152574', '25', 5.0),\n",
       " ('398579', '14240', 5.0),\n",
       " ('455843', '9818', 3.0),\n",
       " ('528287', '6206', 4.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'surprise.prediction_algorithms.predictions.Prediction'>\n",
      "----------------------------------------\n",
      "Prediction Instance\n",
      "----------------------------------------\n",
      "user: 286106     item: 13795      r_ui = 4.00   est = 3.41   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "print(type(predictions[0]))\n",
    "print('-'*40)\n",
    "print('Prediction Instance')\n",
    "print('-'*40)\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can directly compute RMSE from predictions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0394377863252822"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating RMSE \n",
    "from surprise import accuracy\n",
    "accuracy.rmse(predictions, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 If we have Testdata in saperately( not in Testset format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***1.5.2.1 We can predict the rating directly from the library***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6051466666666667"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also ask for individual predictions of the User-Item prediction.\n",
    "algo.estimate(u=str(2422966), i=str(6439))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***1.5.2.2 Getting the ACTUAL RATING from the data with USERID and ITEMID***\n",
    "    - (to compute error ==> RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***ir*** is a DEFAULT DICT of items with lists having tuples of (user, rating)\n",
    "* item and user are in the form of inner_user_id and inner_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_ratings = trainset.ir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets just say, we want the rating of (user, item) from our training set.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, i = '1013034', '14890' # actual ids from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert the **userid and itemid** into **innerIds in **trainingset.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "User 1013034 is not part of the trainset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\trainset.py\u001b[0m in \u001b[0;36mto_inner_uid\u001b[1;34m(self, ruid)\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw2inner_id_users\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mruid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '1013034'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-5b14a5dfd0f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0miid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_inner_iid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Item_Inner_Id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0muid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_inner_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# User_Inner_Id\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\surprise\\trainset.py\u001b[0m in \u001b[0;36mto_inner_uid\u001b[1;34m(self, ruid)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             raise ValueError('User ' + str(ruid) +\n\u001b[1;32m--> 109\u001b[1;33m                              ' is not part of the trainset.')\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_raw_uid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miuid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: User 1013034 is not part of the trainset."
     ]
    }
   ],
   "source": [
    "iid = trainset.to_inner_iid(i) # Item_Inner_Id\n",
    "uid = trainset.to_inner_uid(u)  # User_Inner_Id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We can get all the ***(user, rating)*** pairs who rated that (***movie ie., iid***)\n",
    " \n",
    " - We will convert that into dictionary for faster accesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_rating_dict = dict(item_ratings[iid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will predict it only if it is in our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.knows_item(i),  trainset.knows_user(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***We can Now get the rating of any user who rated that movie..***\n",
    "    * We don''t get any key error, because we are checking before acceing the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1013034'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-3125ef876ac3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mitem_rating_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: '1013034'"
     ]
    }
   ],
   "source": [
    "item_rating_dict[u]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building Our Own Prediction Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- There is a class named **AlgoBase** from which every class in this library is derived from. \n",
    "\n",
    "\n",
    "- It has ***estimate()*** method, which is called by the ***predict(uid, iid)***  it returns ***predicted_rating***.\n",
    "\n",
    "\n",
    "- We Can build our own prediction algorithm from this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import AlgoBase, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurPredictionAlgo_1(AlgoBase):\n",
    "    \n",
    "    # We have to call the __init__ of AlgoBase, It is mandatory, \n",
    "    def __init__(self):    \n",
    "        AlgoBase.__init__(self,)\n",
    "        \n",
    "    def estimate(self, u, i):\n",
    "        # we can write our own predictor.\n",
    "        # we can also return any other info in ***details*** dictionary..\n",
    "        return self.trainset.global_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR, we can makeour own prediction algorithms with similarity or baseline models.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurPredictionAlgo_2(AlgoBase):\n",
    "    \n",
    "    def __init__(self, sim_options={}, bsl_options={}):\n",
    "        \"\"\"If we want to set some parameters of AlgoBase, we can pass like this\"\"\"\n",
    "        # We have to call the __init__ of AlgoBase, It is mandatory, \n",
    "        AlgoBase.__init__(self, sim_options= sim_options,\n",
    "                         bsl_options = bsl_options)\n",
    "    \n",
    "    \n",
    "    def fit(self, trainset):\n",
    "        \"\"\"\n",
    "         we can use this to train our model and some other purposes if you wnat to.\n",
    "         compute the similarities(usr_usr or itm_itm) and baselines (user and item biases)\n",
    "         And we ADD it to our MODEL, which can be used while predicting...\n",
    "        \n",
    "        \"\"\"\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        # Both are computed based on options that we provided at initialization\n",
    "        self.bu, self.bi, self.compute_baselines()\n",
    "        self.sim = self.compute_similarities()\n",
    "    \n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"This code snipper from docs....(github.com/.../examples/..)\n",
    "        1. Compute similarities between u and v, where v describes all other\n",
    "        2. users that have also rated item i.\n",
    "        3. Sort these neighbors by similarity\n",
    "        \"\"\"\n",
    "        \n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible('User and/or item is unkown.')\n",
    "\n",
    "        neighbors = [(v, self.sim[u, v]) for (v, r) in self.trainset.ir[i]]\n",
    "        neighbors = sorted(neighbors, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print('The 3 nearest neighbors of user', str(u), 'are:')\n",
    "        for v, sim_uv in neighbors[:3]:\n",
    "            print('user {0:} with sim {1:1.2f}'.format(v, sim_uv))\n",
    "\n",
    "        # ... Aaaaand return the baseline estimate anyway ;)\n",
    "        bsl = self.trainset.global_mean + self.bu[u] + self.bi[i]\n",
    "        return bsl\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our probe.txt format : \n",
    "\n",
    "- movie_id_1:\n",
    "    - user_id_1\n",
    "    - user_id_2\n",
    "    - ......\n",
    "    - ......\n",
    "- movie_id_2:\n",
    "    - user_id_42\n",
    "    - user_id_15\n",
    "    - ......\n",
    "    - ......\n",
    "- ......\n",
    "    - ......\n",
    "    - ......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load our Actual Netflix prize dataset from our csv file.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our train.csv file is formatted (movie, user, rating, data) and free of duplicates...\n",
    "\n",
    "\n",
    "- We will form our trainset from this train.csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.1 Our train.csv format : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "    - **_movie_id_, _user_id_, _rating_, _date_** ( This line is not present in file..)\n",
    "\n",
    "    -  1        ,  1488844 ,    3.0  , 2005-09-06\n",
    "    -  1        ,   822109 ,    5.0  , 2005-05-13\n",
    "    -  1        ,   885013 ,    4.0  , 2005-10-19\n",
    "    -  1        ,     30878,    4.0  , 2005-12-26\n",
    "    - ........................................\n",
    "    - .................................\n",
    "    - ...........................\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It reorder the format to (user, item, rating [,timestamp])***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfx_reader = Reader(line_format='item user rating timestamp', sep=',')\n",
    "nfx_reader.indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 0:03:20.163452\n"
     ]
    }
   ],
   "source": [
    "# load the data from the file in specified format\n",
    "start = datetime.now()\n",
    "nfx_data = Dataset.load_from_file('train.csv', reader=nfx_reader)\n",
    "print(\"time :\",datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  100479540  tuples(ratings)\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \",len(nfx_data.raw_ratings),\" tuples(ratings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1488844', '1', 3.0, '2005-09-06'),\n",
       " ('822109', '1', 5.0, '2005-05-13'),\n",
       " ('885013', '1', 4.0, '2005-10-19')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfx_data.raw_ratings[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
